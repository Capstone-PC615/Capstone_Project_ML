{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPIlijnZR8dgBJ1NHu+N8Lv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HandersThe/Capstone_Project_ML/blob/master/ProjectCapstone_EfficientNetV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_EFsdi4LIvc"
      },
      "outputs": [],
      "source": [
        "#Import Dependencies\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.applications import EfficientNetV2B0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Kaggle\n",
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "okUf8q_1MaOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can get Kaggle api by creating an account from their website"
      ],
      "metadata": {
        "id": "0iYZIk03XO6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload kaggle api\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "m6b50_5YYumj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create directory for kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "#Download dataset and unzip\n",
        "!kaggle datasets download -d techsash/waste-classification-data\n",
        "!unzip waste-classification-data.zip"
      ],
      "metadata": {
        "id": "cGsRW0sxNMUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The number of output classes\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "#Dataset directory path\n",
        "train_dir = \"dataset/DATASET/TRAIN/\"\n",
        "valid_dir = \"dataset/DATASET/TEST/\"\n",
        "\n",
        "#Init for dataset\n",
        "batch_size = 32\n",
        "img_height = 160\n",
        "img_width = 160"
      ],
      "metadata": {
        "id": "MWuTV29yNTjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training dataset\n",
        "train_ds = keras.utils.image_dataset_from_directory(train_dir,\n",
        "                                                    shuffle=True,\n",
        "                                                    image_size=(img_height, img_width),\n",
        "                                                    batch_size=batch_size)\n",
        "\n",
        "#Validation dataset\n",
        "val_ds = keras.utils.image_dataset_from_directory(valid_dir,\n",
        "                                                  shuffle=True,\n",
        "                                                  image_size=(img_height, img_width),\n",
        "                                                  batch_size=batch_size)"
      ],
      "metadata": {
        "id": "bPa5000WPo-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualise the data\n",
        "class_names = [\"Organic\", \"Inorganic\"]\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "ZyZtWt0RQFjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a Test dataset\n",
        "val_batches = tf.data.experimental.cardinality(val_ds)\n",
        "test_ds = val_ds.take(val_batches // 5)\n",
        "val_ds = val_ds.skip(val_batches // 5)"
      ],
      "metadata": {
        "id": "1ovjrNZXV0pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Configure dataset for performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "-4Cd-ui2Qkvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Callback (use this if you want to increase the number of epochs or make model training automatically stop)\n",
        "#The defaults are 0.95 for accuracy and 0.93 for val_accuracy\n",
        "class myCallback(keras.callbacks.Callback):\n",
        "      def on_epoch_end(self,epoch,logs={}):\n",
        "        if((logs.get('accuracy')>0.95) and (logs.get('val_accuracy')>0.93)):\n",
        "          self.model.stop_training = True"
      ],
      "metadata": {
        "id": "0tRjmWVua7ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data augmentation\n",
        "data_augmentation = Sequential([\n",
        "  layers.RandomRotation(factor=0.15),\n",
        "  layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
        "  layers.RandomFlip(),\n",
        "  layers.RandomContrast(factor=0.1),\n",
        "])\n",
        "\n",
        "#Preprocess Input\n",
        "preprocess_input = keras.applications.efficientnet_v2.preprocess_input"
      ],
      "metadata": {
        "id": "axdwX5kbNMei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model build\n",
        "inputs = keras.Input(shape=(img_height, img_width, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "#Base Model\n",
        "base_model = EfficientNetV2B0(input_tensor=x,\n",
        "                              include_top=False,\n",
        "                              weights=\"imagenet\")\n",
        "\n",
        "#Freeze the pretrained weights\n",
        "base_model.trainable = False\n",
        "\n",
        "x = layers.GlobalAveragePooling2D()(base_model.output)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "outputs = layers.Dense(2)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "#Model compile\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "VCa4BncXRBL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "g5YWNw1FNVOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initial model evaluation\n",
        "loss0, accuracy0 = model.evaluate(val_ds)"
      ],
      "metadata": {
        "id": "MCgbOhLREpIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training epochs\n",
        "initial_epochs=3\n",
        "\n",
        "#Model fit\n",
        "history = model.fit(train_ds,\n",
        "                    validation_data=val_ds,\n",
        "                    epochs=initial_epochs\n",
        ")"
      ],
      "metadata": {
        "id": "2ovHlkJ2Ratn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualise training results\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rm4wB75RTNK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fine tuning\n",
        "#Take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "#Unfreeze the top 20 layers while leaving BatchNorm layers frozen\n",
        "for layer in base_model.layers[-20:]:\n",
        "  if not isinstance(layer, layers.BatchNormalization):\n",
        "            layer.trainable = True\n",
        "\n",
        "##Or you can use this\n",
        "#Fine-tune from this layer onwards\n",
        "#fine_tune_at = 100\n",
        "\n",
        "#Freeze all the layers before the `fine_tune_at` layer\n",
        "#for layer in base_model.layers[:fine_tune_at]:\n",
        "#  layer.trainable = False\n",
        "\n",
        "#Fine tuning model compile\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "I6exzBW8Gk3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fine tuning model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Z_VTIrx7G3wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fine tune epochs\n",
        "fine_tune_epochs = 3\n",
        "total_epochs =  initial_epochs + fine_tune_epochs\n",
        "\n",
        "#Fine tuning model fit\n",
        "history_fine = model.fit(train_ds,\n",
        "                    validation_data=val_ds,\n",
        "                    epochs=total_epochs\n",
        ")"
      ],
      "metadata": {
        "id": "_E4KZ2u9Hg1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualise fine tunned model\n",
        "acc += history_fine.history['accuracy']\n",
        "val_acc += history_fine.history['val_accuracy']\n",
        "\n",
        "loss += history_fine.history['loss']\n",
        "val_loss += history_fine.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.ylim([0.8, 1])\n",
        "plt.plot([initial_epochs-1,initial_epochs-1],\n",
        "          plt.ylim(), label='Start Fine Tuning')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.ylim([0, 1.0])\n",
        "plt.plot([initial_epochs-1,initial_epochs-1],\n",
        "         plt.ylim(), label='Start Fine Tuning')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5dpmpqLrKld9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test dataset model evaluation\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print('Test accuracy :', accuracy)"
      ],
      "metadata": {
        "id": "-EvE0Z5685Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict one uploaded images\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "\n",
        "  path = fn\n",
        "  img = tf.keras.utils.load_img(path,\n",
        "                                target_size=(img_height, img_width)\n",
        "  )\n",
        "  imgplot = plt.imshow(img)\n",
        "  x = tf.keras.utils.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  logits = model.predict(images)\n",
        "  softmax = tf.nn.softmax(logits[0])\n",
        "\n",
        "\n",
        "  print(\n",
        "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
        "    .format(class_names[np.argmax(softmax)], 100 * np.max(softmax))\n",
        "  )"
      ],
      "metadata": {
        "id": "8uQPcNObLFzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model save directory\n",
        "model_save_location = \"/content/Model/TrashSort\""
      ],
      "metadata": {
        "id": "-yHbnuqB2x5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save Keras model\n",
        "model.save(model_save_location)"
      ],
      "metadata": {
        "id": "mhZrhmeCCu79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Optimise prediction for GCP Vertex AI (don't use this if you want to use TF Lite)\n",
        "CONCRETE_INPUT = \"numpy_inputs\"\n",
        "\n",
        "\n",
        "def _preprocess(bytes_input):\n",
        "    decoded = tf.io.decode_jpeg(bytes_input, channels=3)\n",
        "    decoded = tf.image.convert_image_dtype(decoded, tf.uint8)\n",
        "    resized = tf.image.resize(decoded, size=(img_height, img_width))\n",
        "    return resized\n",
        "\n",
        "\n",
        "@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
        "def preprocess_fn(bytes_inputs):\n",
        "    decoded_images = tf.map_fn(\n",
        "        _preprocess, bytes_inputs, dtype=tf.float32, back_prop=False\n",
        "    )\n",
        "    return {\n",
        "        CONCRETE_INPUT: decoded_images\n",
        "    }  # User needs to make sure the key matches model's input\n",
        "\n",
        "\n",
        "@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
        "def serving_fn(bytes_inputs):\n",
        "    images = preprocess_fn(bytes_inputs)\n",
        "    prob = m_call(**images)\n",
        "    return prob\n",
        "\n",
        "\n",
        "m_call = tf.function(model.call).get_concrete_function(\n",
        "    [tf.TensorSpec(shape=[None, img_height, img_width, 3], dtype=tf.float32, name=CONCRETE_INPUT)]\n",
        ")\n",
        "\n",
        "tf.saved_model.save(model, model_save_location, signatures={\"serving_default\": serving_fn})"
      ],
      "metadata": {
        "id": "Lvk3LAMIPk55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For android apps (This model has bigger size than the one in EfficientNetLite notebook)\n",
        "#Converting to TF Lite\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(model_save_location)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "#Writing TF Lite model\n",
        "with open(\"model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_quant_model)"
      ],
      "metadata": {
        "id": "mUtJa-87eZPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download Model\n",
        "!zip -r TrashSort.zip /content/Model/TrashSort"
      ],
      "metadata": {
        "id": "9B9cWWkYekrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please refer to https://codelabs.developers.google.com/vertex-image-deploy and https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_tf_serving_function.ipynb for setting up GCP Vertex AI enviroment"
      ],
      "metadata": {
        "id": "oocFlCphV8sF"
      }
    }
  ]
}